{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2_part_a",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neamJoGNEVWl"
      },
      "source": [
        "# Import Libraries to Use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxDA0cq-ELbm"
      },
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers, models\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LaDe8P0EZyO"
      },
      "source": [
        "# Load Dataset to Use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6P584vuQCky"
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "epochs = 1000  # Fixed\n",
        "batch_size = 128  # Fixed\n",
        "learning_rate = 0.001\n",
        "use_dropout = False  # Default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWb6fa37EbSP"
      },
      "source": [
        "# Fixed, no need change\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        try:\n",
        "            samples = pickle.load(fo)\n",
        "        except UnicodeDecodeError:  # python 3.x\n",
        "            fo.seek(0)\n",
        "            samples = pickle.load(fo, encoding='latin1')\n",
        "\n",
        "    data, labels = samples['data'], samples['labels']\n",
        "\n",
        "    data = np.array(data, dtype=np.float32) / 255\n",
        "    labels = np.array(labels, dtype=np.int32)\n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLwkMctYE1GN"
      },
      "source": [
        "# Training and test\n",
        "x_train, y_train = load_data('data_batch_1')\n",
        "x_test, y_test = load_data('test_batch_trim')\n",
        "\n",
        "x_train = np.reshape(x_train,(x_train.shape[0],3,32,32)).transpose(0,2,3,1)\n",
        "x_test = np.reshape(x_test,(x_test.shape[0],3,32,32)).transpose(0,2,3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "900rcsy5HrZn"
      },
      "source": [
        "# Create folder to store models and results\n",
        "if not os.path.exists('./models'):\n",
        "    os.mkdir('./models')\n",
        "if not os.path.exists('./results'):\n",
        "    os.mkdir('./results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8pe0xJeFhoY"
      },
      "source": [
        "# Make the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbCyHKqP-B7g"
      },
      "source": [
        "def make_model(num_ch_c1, num_ch_c2, use_dropout):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Input(shape=(32, 32, 3)))\n",
        "  # model.add(layers.Reshape(target_shape=(32, 32, 3), input_shape=(3072,)))\n",
        "  model.add(layers.Conv2D(num_ch_c1, 9, activation='relu', padding='valid', input_shape=(None, None, 3)))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "  model.add(layers.Conv2D(num_ch_c2, 5, activation='relu', padding='valid', input_shape=(None, None, 3)))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "  model.add(layers.Flatten())\n",
        "  if use_dropout:\n",
        "    model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(300, use_bias=True))\n",
        "  if use_dropout:\n",
        "    model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(10, use_bias=True, input_shape=(300,)))  # Here no softmax because we have combined it with the loss\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4wsKApcXknm"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAQG8hlK0gyo"
      },
      "source": [
        "# Set channel sizes\n",
        "num_ch_c1 = 50\n",
        "num_ch_c2 = 60\n",
        "\n",
        "# Create folder to store models and results\n",
        "if not os.path.exists('./results/parta/q1'):\n",
        "    os.mkdir('./results/parta/q1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "374UA46bxtpA"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC4ius8gHbFf"
      },
      "source": [
        "q1_model = make_model(num_ch_c1, num_ch_c2, use_dropout=False)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "# Training\n",
        "q1_model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
        "q1_history = q1_model.fit(x_train,\n",
        "                       y_train,\n",
        "                       batch_size=batch_size,\n",
        "                       epochs=epochs,\n",
        "                       validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "# Saving history to file\n",
        "print('Saving history to file')\n",
        "filename = './results/parta/q1/q1_history'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "    pickle.dump(q1_history.history, file_pi)\n",
        "print('file saved at {}'.format(filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLbSjV2hxnS-"
      },
      "source": [
        "## Find Max and Final Validation Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpX8QhNOYwod"
      },
      "source": [
        "# Find Max Accuracy and Final Accuracy\n",
        "max_acc = np.amax(q1_history.history['val_accuracy'])\n",
        "final_acc = q1_history.history['val_accuracy'][epochs-1]\n",
        "print('Max Val Acc: {} \\t Final Val Acc: {}'.format(max_acc, final_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ENL4Ar4xyCB"
      },
      "source": [
        "## Plot and Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm0jhjDX3iq2"
      },
      "source": [
        "model = 'Part A Q1'\n",
        "fig = plt.figure(1, figsize=(12,4))\n",
        "\n",
        "train_loss = q1_history.history['loss']\n",
        "val_loss = q1_history.history['val_loss']\n",
        "train_acc = q1_history.history['accuracy']\n",
        "val_acc = q1_history.history['val_accuracy']\n",
        "\n",
        "# Subplot 1\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Train')\n",
        "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Test')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.title('Loss for {}'.format(model, fontsize=14))\n",
        "\n",
        "# Subplot 2\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(train_acc) + 1), train_acc, label='Train')\n",
        "plt.plot(range(1, len(val_acc) + 1), val_acc, label='Test')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.title('Accuracy for {}'.format(model, fontsize=14))\n",
        "\n",
        "plt.savefig('./results/parta/q1/q1_plot.pdf')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4BIDd_GAu8-"
      },
      "source": [
        "## Plot Feature Map for first 2 test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3QUs5c_Aw-E"
      },
      "source": [
        "# Extracts the outputs of the C1,S1,C2,S2 layers\n",
        "layer_outputs = [layer.output for layer in q1_model.layers[:4]] \n",
        "\n",
        "# Extract the outputs of C1,S1,C2,S2 layer\n",
        "activation_model = keras.models.Model(inputs=q1_model.input, \n",
        "                                      outputs=layer_outputs)\n",
        "\n",
        "activations_1 = activation_model.predict(np.reshape(x_test[0], (-1,32,32,3)))\n",
        "activations_2 = activation_model.predict(np.reshape(x_test[1], (-1,32,32,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2i6MsZTzWhW"
      },
      "source": [
        "layer_names = ['C1', 'S1', 'C2', 'S2']\n",
        "for num, (layer, activation, name) in enumerate(zip(q1_model.layers[:4], \n",
        "                                                    activations_1, \n",
        "                                                    layer_names)):\n",
        "  # Plot activation layer for x_test[0]\n",
        "  fig = plt.figure(num=num, figsize=(7,5))\n",
        "  # plt.gray()\n",
        "  channels = activation.shape[3]\n",
        "  rows = channels/10\n",
        "  for i in range(channels):\n",
        "    plt.subplot(rows, 10, i+1); plt.axis('off'); plt.imshow(activation[0,:,:,i])\n",
        "    fig.suptitle('Feature Map for {}({})'.format(layer.name, name), fontsize=14)\n",
        "    plt.savefig('./results/parta/q1/feature_map_test{}_{}.pdf'.format('0', name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXjFoVem3ZHP"
      },
      "source": [
        "layer_names = ['C1', 'S1', 'C2', 'S2']\n",
        "for num, (layer, activation, name) in enumerate(zip(q1_model.layers[:4], \n",
        "                                                    activations_2, \n",
        "                                                    layer_names)):\n",
        "  # Plot activation layer for x_test[1]\n",
        "  fig = plt.figure(num=num, figsize=(7,5))\n",
        "  # plt.gray()\n",
        "  channels = activation.shape[3]\n",
        "  rows = channels/10\n",
        "  for i in range(channels):\n",
        "    plt.subplot(rows, 10, i+1); plt.axis('off'); plt.imshow(activation[0,:,:,i])\n",
        "    fig.suptitle('Feature Map for {}({})'.format(layer.name, name), fontsize=14)\n",
        "    plt.savefig('./results/parta/q1/feature_map_test{}_{}.pdf'.format('1', name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FnHZ3H3rTgI"
      },
      "source": [
        "# Save the first 2 test images\n",
        "for i in range(2):\n",
        "  x_img = x_test[i].astype(np.float32)\n",
        "  plt.imshow(x_img)\n",
        "  plt.title('x_test[{}]'.format(str(i)))\n",
        "  plt.savefig('./results/parta/q1/test{}_image.pdf'.format(str(i)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWL2g8ANC52g"
      },
      "source": [
        "# Question 2: Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EXBDEH4zJ6V"
      },
      "source": [
        "# Create folder to store models and results\n",
        "if not os.path.exists('./results/parta/q2'):\n",
        "    os.mkdir('./results/parta/q2')\n",
        "\n",
        "ch1_list = [10,30,50,70,90]\n",
        "ch2_list = [20,40,60,80,100]\n",
        "combi = []\n",
        "histories = {}\n",
        "\n",
        "epochs = 1000  # Fixed\n",
        "batch_size = 128  # Fixed\n",
        "learning_rate = 0.001\n",
        "use_dropout = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD_Ro7liC6ya"
      },
      "source": [
        "for ch1 in ch1_list:\n",
        "  for ch2 in ch2_list:\n",
        "    print('Running for ch1 = {}, ch2 ={}'.format(ch1, ch2))\n",
        "    title = '{}_{}'.format(ch1, ch2)\n",
        "    combi.append(title)\n",
        "    model = make_model(ch1, ch2, use_dropout)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
        "    histories[title] = model.fit(x_train,\n",
        "                                 y_train,\n",
        "                                 batch_size=batch_size,\n",
        "                                 epochs=epochs,\n",
        "                                 validation_data=(x_test, y_test),\n",
        "                                 verbose=0)\n",
        "    print('Training complete for ch1 = {}, ch2 ={}'.format(ch1, ch2))\n",
        "    # Saving history to file\n",
        "    print('Saving history to file')\n",
        "    filename = './results/parta/q2/gridsearch_{}'.format(title)\n",
        "    with open(filename, 'wb') as file_pi:\n",
        "        pickle.dump(histories[title].history, file_pi)\n",
        "    print('file saved at {}'.format(filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLT5Tg_GCaSI"
      },
      "source": [
        "val_acc_df = pd.DataFrame(columns=['model', 'final_val_acc', 'max_val_acc'])\n",
        "\n",
        "# Find the last and max val_accuracy for all models\n",
        "for model in histories:\n",
        "  final_val_acc = histories[model].history['val_accuracy'][-1]\n",
        "  max_val_acc = np.amax(histories[model].history['val_accuracy'])\n",
        "  val_acc_df = val_acc_df.append({'model': model,\n",
        "                                  'final_val_acc': final_val_acc,\n",
        "                                  'max_val_acc': max_val_acc}, ignore_index=True)\n",
        "\n",
        "val_acc_df.to_csv('./results/parta/q2/gridsearch_results.csv', index=False)\n",
        "val_acc_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vK7FFmc1CjT"
      },
      "source": [
        "# Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZDuGY9xivnS"
      },
      "source": [
        "## Using optimal combination of ch1 = 70 and ch2 = 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jABCB1yzlnx2"
      },
      "source": [
        "# Create folder to store models and results\n",
        "if not os.path.exists('./results/parta/q3'):\n",
        "    os.mkdir('./results/parta/q3')\n",
        "\n",
        "opt_ch1 = 70\n",
        "opt_ch2 = 40\n",
        "\n",
        "oc_histories = {}\n",
        "\n",
        "epochs = 1000  # Fixed\n",
        "batch_size = 128  # Fixed\n",
        "learning_rate = 0.001\n",
        "optimizer_ = 'SGD'  # Question 3\n",
        "use_dropout = False  # Question 3(d) (see make_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fjbYG9LkDLz"
      },
      "source": [
        "### Part a: Adding momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja7d01PEiuia"
      },
      "source": [
        "# Adding momentum to SGD optimizer\n",
        "momentum = 0.1\n",
        "title = 'momentum'\n",
        "\n",
        "momentum_opt = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.1)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "mom_model = make_model(opt_ch1, opt_ch2, use_dropout)\n",
        "\n",
        "mom_model.compile(optimizer=momentum_opt, loss=loss, metrics='accuracy')\n",
        "oc_histories[title] = mom_model.fit(x_train,\n",
        "                                    y_train,\n",
        "                                    batch_size=batch_size,\n",
        "                                    epochs=epochs,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    verbose=2)\n",
        "\n",
        "filename = './results/parta/q3/history_{}'.format(title)\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(oc_histories[title].history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6fFBGPFoA7S"
      },
      "source": [
        "### Part b: Using RMSProp algorithm for learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icmd6FgfoFrf"
      },
      "source": [
        "# Using RMSProp algorithm for learning\n",
        "title = 'rmsprop'\n",
        "\n",
        "rmsp_opt = keras.optimizers.RMSprop(learning_rate=learning_rate, epsilon=1)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "rmsp_model = make_model(opt_ch1, opt_ch2, use_dropout)\n",
        "rmsp_model.compile(optimizer=rmsp_opt, loss=loss, metrics='accuracy')\n",
        "\n",
        "oc_histories[title] = rmsp_model.fit(x_train,\n",
        "                                        y_train,\n",
        "                                        batch_size=batch_size,\n",
        "                                        epochs=epochs,\n",
        "                                        validation_data=(x_test, y_test),\n",
        "                                        verbose=2)\n",
        "\n",
        "\n",
        "filename = './results/parta/q3/history_{}'.format(title)\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(oc_histories[title].history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZvF8ROJorRu"
      },
      "source": [
        "### Part c: Using Adam Optimizer for learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj4KDNm5owQF"
      },
      "source": [
        "# Using Adam optimizer for learning\n",
        "title = 'adam'\n",
        "use_dropout = False\n",
        "\n",
        "adam_opt = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "adam_model = make_model(opt_ch1, opt_ch2, use_dropout)\n",
        "adam_model.compile(optimizer=adam_opt, loss=loss, metrics='accuracy')\n",
        "\n",
        "oc_histories[title] = adam_model.fit(x_train,\n",
        "                                   y_train,\n",
        "                                   batch_size=batch_size,\n",
        "                                   epochs=epochs,\n",
        "                                   validation_data=(x_test, y_test),\n",
        "                                   verbose=2)\n",
        "\n",
        "filename = './results/parta/q3/history_{}'.format(title)\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(oc_histories[title].history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJw1FrubpIZE"
      },
      "source": [
        "### Part d: Adding dropout=0.5 to the 2 fully connected layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQIv3jg9pOB8"
      },
      "source": [
        "# Adding dropout=0.5 to the 2 fully connected layers\n",
        "title = 'dropout'\n",
        "use_dropout = True\n",
        "\n",
        "dropout_opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "dropout_model = make_model(opt_ch1, opt_ch2, use_dropout)\n",
        "dropout_model.compile(optimizer=dropout_opt, loss=loss, metrics='accuracy')\n",
        "\n",
        "oc_histories[title] = dropout_model.fit(x_train,\n",
        "                                   y_train,\n",
        "                                   batch_size=batch_size,\n",
        "                                   epochs=epochs,\n",
        "                                   validation_data=(x_test, y_test),\n",
        "                                   verbose=2)\n",
        "\n",
        "filename = './results/parta/q3/history_{}'.format(title)\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(oc_histories[title].history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0_UGyg90SIt"
      },
      "source": [
        "## Plot results for all models in Q3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7VcROho1IB3"
      },
      "source": [
        "# Plot all figures\n",
        "for i, model in enumerate(oc_histories):\n",
        "  fig = plt.figure(num=i, figsize=(12,4))\n",
        "\n",
        "  # Subplot 1\n",
        "  plt.subplot(1, 2, 1)\n",
        "\n",
        "  train_loss = oc_histories[model].history['loss']\n",
        "  val_loss = oc_histories[model].history['val_loss']\n",
        "\n",
        "  plt.plot(range(1, len(train_loss) + 1), train_loss, label='Train')\n",
        "  plt.plot(range(1, len(val_loss) + 1), val_loss, label='Test')\n",
        "\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.title('Loss for {}'.format(model, fontsize=14))\n",
        "  \n",
        "  # Subplot 2\n",
        "  plt.subplot(1, 2, 2)\n",
        "\n",
        "  train_acc = oc_histories[model].history['accuracy']\n",
        "  val_acc = oc_histories[model].history['val_accuracy']\n",
        "\n",
        "  plt.plot(range(1, len(train_acc) + 1), train_acc, label='Train')\n",
        "  plt.plot(range(1, len(val_acc) + 1), val_acc, label='Test')\n",
        "  \n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.title('Accuracy for {}'.format(model, fontsize=14))\n",
        "  plt.savefig('./results/parta/q3/q3_{}.pdf'.format(model))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX6db_pIaAP4"
      },
      "source": [
        "for model in oc_histories:\n",
        "  final_val_acc = oc_histories[model].history['val_accuracy'][-1]\n",
        "  max_val_acc = np.amax(oc_histories[model].history['val_accuracy'])\n",
        "  print('model: {} \\t final_val_acc: {} \\t max_val_acc: {}'.format(model, final_val_acc, max_val_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UGGaX8-KKG9"
      },
      "source": [
        "# Question 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0dIk1Do9xBT"
      },
      "source": [
        "for model in oc_histories:\n",
        "  final_val_acc = oc_histories[model].history['val_accuracy'][-1]\n",
        "  max_val_acc = np.amax(oc_histories[model].history['val_accuracy'])\n",
        "  val_acc_df = val_acc_df.append({'model': model,\n",
        "                                  'final_val_acc': final_val_acc,\n",
        "                                  'max_val_acc': max_val_acc}, ignore_index=True)\n",
        "\n",
        "# append Q1\n",
        "final_val_acc = q1_history.history['val_accuracy'][-1]\n",
        "max_val_acc = np.amax(q1_history.history['val_accuracy'])\n",
        "val_acc_df = val_acc_df.append({'model': '50_60_Q1',\n",
        "                                'final_val_acc': final_val_acc,\n",
        "                                'max_val_acc': max_val_acc}, ignore_index=True)\n",
        "\n",
        "val_acc_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
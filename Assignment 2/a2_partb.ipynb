{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NN Part B.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph_8kHxdduoR"
      },
      "source": [
        "# Import Libraries to Use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGHLfrhZdpx7"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "import csv\n",
        "import re\n",
        "import pylab\n",
        "from time import time\n",
        "import pickle\n",
        "\n",
        "import collections\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "seed = 10\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Create folder to store results\n",
        "if not os.path.exists('./results/partb'):\n",
        "    os.mkdir('./results/partb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXEK5a-wd3ZB"
      },
      "source": [
        "# Set Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05YZy_L7d1Hp"
      },
      "source": [
        "batch_size = 128\n",
        "one_hot_size = 256\n",
        "no_epochs = 250\n",
        "lr = 0.01\n",
        "\n",
        "train_file = 'train_medium.csv'\n",
        "test_file = 'test_medium.csv'\n",
        "\n",
        "MAX_DOCUMENT_LENGTH = 100\n",
        "N_FILTERS = 10\n",
        "FILTER_SHAPE1 = [20, 256]\n",
        "FILTER_SHAPE2 = [20, 1]\n",
        "POOLING_WINDOW = 4\n",
        "POOLING_STRIDE = 2\n",
        "MAX_LABEL = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njI0NU9WeBi6"
      },
      "source": [
        "# Read Data with Character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om56Ajpvd4-k"
      },
      "source": [
        "def vocabulary(strings):\n",
        "    chars = sorted(list(set(list(''.join(strings)))))\n",
        "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "    vocab_size = len(chars)\n",
        "    return vocab_size, char_to_ix\n",
        "\n",
        "def preprocess(strings, char_to_ix, MAX_LENGTH):\n",
        "    data_chars = [list(d.lower()) for _, d in enumerate(strings)]\n",
        "    for i, d in enumerate(data_chars):\n",
        "        if len(d)>MAX_LENGTH:\n",
        "            d = d[:MAX_LENGTH]\n",
        "        elif len(d) < MAX_LENGTH:\n",
        "            d += [' '] * (MAX_LENGTH - len(d))\n",
        "            \n",
        "    data_ids = np.zeros([len(data_chars), MAX_LENGTH], dtype=np.int64)\n",
        "    for i in range(len(data_chars)):\n",
        "        for j in range(MAX_LENGTH):\n",
        "            data_ids[i, j] = char_to_ix[data_chars[i][j]]\n",
        "    return np.array(data_ids)\n",
        "\n",
        "def read_data_chars(train_file, test_file):\n",
        "    x_train, y_train, x_test, y_test = [], [], [], []\n",
        "    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n",
        "    with open(train_file, encoding='utf-8') as filex:\n",
        "        reader = csv.reader(filex)\n",
        "        for row in reader:\n",
        "            data = cop.sub(\"\", row[1])\n",
        "            x_train.append(data)\n",
        "            y_train.append(int(row[0]))\n",
        "\n",
        "    with open(test_file, encoding='utf-8') as filex:\n",
        "        reader = csv.reader(filex)\n",
        "        for row in reader:\n",
        "            data = cop.sub(\"\", row[1])\n",
        "            x_test.append(data)\n",
        "            y_test.append(int(row[0]))\n",
        "\n",
        "\n",
        "    vocab_size, char_to_ix = vocabulary(x_train+x_test)\n",
        "    x_train = preprocess(x_train, char_to_ix, MAX_DOCUMENT_LENGTH)\n",
        "    y_train = np.array(y_train)\n",
        "    x_test = preprocess(x_test, char_to_ix, MAX_DOCUMENT_LENGTH)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    x_train = tf.constant(x_train, dtype=tf.int64)\n",
        "    y_train = tf.constant(y_train, dtype=tf.int64)\n",
        "    x_test = tf.constant(x_test, dtype=tf.int64)\n",
        "    y_test = tf.constant(y_test, dtype=tf.int64)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# Read in characters files\n",
        "x_train_c, y_train_c, x_test_c, y_test_c = read_data_chars(train_file, test_file)\n",
        "\n",
        "# Use `tf.data` to batch and shuffle the dataset:\n",
        "train_ds_c = tf.data.Dataset.from_tensor_slices((x_train_c, y_train_c)).shuffle(10000).batch(batch_size)\n",
        "test_ds_c = tf.data.Dataset.from_tensor_slices((x_test_c, y_test_c)).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG2KLX1HrtQ3"
      },
      "source": [
        "# Read Data with Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6od8zFZrynH"
      },
      "source": [
        "def clean_str(text):\n",
        "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def build_word_dict(contents):\n",
        "    words = list()\n",
        "    for content in contents:\n",
        "        for word in word_tokenize(clean_str(content)):\n",
        "            words.append(word)\n",
        "\n",
        "    word_counter = collections.Counter(words).most_common()\n",
        "    word_dict = dict()\n",
        "    word_dict[\"<pad>\"] = 0\n",
        "    word_dict[\"<unk>\"] = 1\n",
        "    word_dict[\"<eos>\"] = 2\n",
        "    for word, _ in word_counter:\n",
        "        word_dict[word] = len(word_dict)\n",
        "    return word_dict\n",
        "\n",
        "\n",
        "def preprocess(contents, word_dict, document_max_len):\n",
        "    x = list(map(lambda d: word_tokenize(clean_str(d)), contents))\n",
        "    x = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[\"<unk>\"]), d)), x))\n",
        "    x = list(map(lambda d: d + [word_dict[\"<eos>\"]], x))\n",
        "    x = list(map(lambda d: d[:document_max_len], x))\n",
        "    x = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[\"<pad>\"]], x))\n",
        "    return x\n",
        "\n",
        "\n",
        "def read_data_words(train_file, test_file):\n",
        "    x_train, y_train, x_test, y_test = [], [], [], []\n",
        "    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n",
        "    with open(train_file, encoding='utf-8') as filex:\n",
        "        reader = csv.reader(filex)\n",
        "        for row in reader:\n",
        "            data = cop.sub(\"\", row[1])\n",
        "            x_train.append(data)\n",
        "            y_train.append(int(row[0]))\n",
        "\n",
        "    with open(test_file, encoding='utf-8') as filex:\n",
        "        reader = csv.reader(filex)\n",
        "        for row in reader:\n",
        "            data = cop.sub(\"\", row[1])\n",
        "            x_test.append(data)\n",
        "            y_test.append(int(row[0]))\n",
        "\n",
        "    word_dict = build_word_dict(x_train+x_test)\n",
        "    x_train = preprocess(x_train, word_dict, MAX_DOCUMENT_LENGTH)\n",
        "    y_train = np.array(y_train)\n",
        "    x_test = preprocess(x_test, word_dict, MAX_DOCUMENT_LENGTH)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    x_train = [x[:MAX_DOCUMENT_LENGTH] for x in x_train]\n",
        "    x_test = [x[:MAX_DOCUMENT_LENGTH] for x in x_test]\n",
        "    x_train = tf.constant(x_train, dtype=tf.int64)\n",
        "    y_train = tf.constant(y_train, dtype=tf.int64)\n",
        "    x_test = tf.constant(x_test, dtype=tf.int64)\n",
        "    y_test = tf.constant(y_test, dtype=tf.int64)\n",
        "\n",
        "    vocab_size = tf.get_static_value(tf.reduce_max(x_train))\n",
        "    vocab_size = max(vocab_size, tf.get_static_value(tf.reduce_max(x_test))) + 1\n",
        "    return x_train, y_train, x_test, y_test, vocab_size\n",
        "\n",
        "\n",
        "x_train, y_train, x_test, y_test, vocab_size = read_data_words(train_file, test_file)\n",
        "# Use `tf.data` to batch and shuffle the dataset:\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JWajJbifBJd"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tOpBPi0fEKz"
      },
      "source": [
        "## CharCNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA1nLhlj1ssc"
      },
      "source": [
        "if not os.path.exists('./results/partb/char_cnn'):\n",
        "    os.mkdir('./results/partb/char_cnn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yylj35nfB9-"
      },
      "source": [
        "# Build model\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "class CharCNN(Model):\n",
        "    def __init__(self, vocab_size=256, use_dropout=False):\n",
        "        super(CharCNN, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.use_dropout = use_dropout\n",
        "        # Weight variables and RNN cell\n",
        "        # 1st convolution and pooling layers\n",
        "        self.conv1 = layers.Conv2D(N_FILTERS, FILTER_SHAPE1, padding='VALID', activation='relu', use_bias=True)\n",
        "        self.pool1 = layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
        "        # 2nd convolution and pooling layers\n",
        "        self.conv2 = layers.Conv2D(N_FILTERS, FILTER_SHAPE2, padding='VALID', activation='relu', use_bias=True)\n",
        "        self.pool2 = layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.dense = layers.Dense(MAX_LABEL, activation='softmax')\n",
        "\n",
        "    def call(self, x, drop_rate=0.5):\n",
        "        # forward\n",
        "        # x format - [n_samples, n_features]\n",
        "        x = tf.one_hot(x, one_hot_size) # x - [n_samples, n_features, one_hot_size (256)]\n",
        "        x = x[..., tf.newaxis]          # x - [n_samples, n_features, one_hot_size (256), channels (1)]\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        if self.use_dropout:\n",
        "          x = tf.nn.dropout(x, drop_rate)\n",
        "        logits = self.dense(x)\n",
        "        return logits\n",
        "\n",
        "char_cnn_model = CharCNN(256, False)\n",
        "char_cnn_dropout = CharCNN(256, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fd4M9mE07-j"
      },
      "source": [
        "### Optimiser and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXF5zI__gbNf"
      },
      "source": [
        "# Choose optimizer and loss function for training\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model. \n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGU7-5lr0_uQ"
      },
      "source": [
        "### Training and Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYszTqJmgkqh"
      },
      "source": [
        "# Training function\n",
        "def train_step(model, x, label, drop_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        out = model(x, drop_rate)\n",
        "        loss = loss_object(label, out)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, out)\n",
        "\n",
        "# Testing function\n",
        "def test_step(model, x, label, drop_rate=0):\n",
        "    out = model(x,drop_rate)\n",
        "    t_loss = loss_object(label, out)\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(label, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9pAXEet1E4t"
      },
      "source": [
        "### Run without dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI9bVQnHg4r_"
      },
      "source": [
        "# without dropout\n",
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds_c:\n",
        "        train_step(char_cnn_model, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds_c:\n",
        "        test_step(char_cnn_model, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "    \n",
        "# save results\n",
        "char_cnn_res = {'train_ec': train_ec,\n",
        "                        'train_acc': train_acc,\n",
        "                        'test_ec': test_ec,\n",
        "                        'test_acc': test_acc,\n",
        "                        'timings': timings}\n",
        "filename = './results/partb/char_cnn/char_cnn_base'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(char_cnn_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh-ivxqy2nlr"
      },
      "source": [
        "# Plot test accuracy\n",
        "pylab.figure(figsize=(12,4))\n",
        "pylab.subplot(1,2,1)\n",
        "pylab.plot(np.arange(no_epochs), test_acc, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_acc, label='train')\n",
        "pylab.title('Model Accuracy for CharCNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('accuracy')\n",
        "pylab.legend(loc='lower right')\n",
        "\n",
        "pylab.subplot(1,2,2)\n",
        "pylab.plot(np.arange(no_epochs), test_ec, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_ec, label='train')\n",
        "pylab.title('Model Loss for CharCNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('entropy cost')\n",
        "pylab.legend(loc='upper right')\n",
        "\n",
        "pylab.savefig('/results/partb/char_cnn/char_cnn_base.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crvxY20P1HgE"
      },
      "source": [
        "### Run with Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZtVbQq00aE5"
      },
      "source": [
        "# with dropout\n",
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds_c:\n",
        "        train_step(char_cnn_dropout, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds_c:\n",
        "        test_step(char_cnn_dropout, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "    \n",
        "# save results\n",
        "char_cnn_dropout_res = {'train_ec': train_ec,\n",
        "                        'train_acc': train_acc,\n",
        "                        'test_ec': test_ec,\n",
        "                        'test_acc': test_acc,\n",
        "                        'timings': timings}\n",
        "filename = './results/partb/char_cnn/char_cnn_dropout'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(char_cnn_dropout_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7bPsbJ8jN_I"
      },
      "source": [
        "# Plot test accuracy\n",
        "pylab.figure(figsize=(12,4))\n",
        "pylab.subplot(1,2,1)\n",
        "pylab.plot(np.arange(no_epochs), test_acc, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_acc, label='train')\n",
        "pylab.title('Model Accuracy for CharCNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('accuracy')\n",
        "pylab.legend(loc='lower right')\n",
        "\n",
        "pylab.subplot(1,2,2)\n",
        "pylab.plot(np.arange(no_epochs), test_ec, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_ec, label='train')\n",
        "pylab.title('Model Loss for CharCNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('entropy cost')\n",
        "pylab.legend(loc='upper right')\n",
        "\n",
        "pylab.savefig('./results/partb/char_cnn/char_cnn_dropout.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTXeku-ZmMpy"
      },
      "source": [
        "## WordCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzsygcy0mwZ_"
      },
      "source": [
        "MAX_DOCUMENT_LENGTH = 100\n",
        "N_FILTERS = 10\n",
        "EMBEDDING_SIZE = 20\n",
        "FILTER_SHAPE1 = [20, 20]\n",
        "FILTER_SHAPE2 = [20, 1]\n",
        "POOLING_WINDOW = 4\n",
        "POOLING_STRIDE = 2\n",
        "MAX_LABEL = 15\n",
        "\n",
        "batch_size = 128\n",
        "no_epochs = 250\n",
        "lr = 0.01\n",
        "\n",
        "if not os.path.exists('./results/partb/word_cnn'):\n",
        "    os.mkdir('./results/partb/word_cnn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6LLlUD2igU"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFHdEmr3npWb"
      },
      "source": [
        "# Build model\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "class WordCNN(Model):\n",
        "    def __init__(self, vocab_size, use_dropout=False):\n",
        "        super(WordCNN, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.use_dropout = use_dropout\n",
        "        self.embedding = layers.Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_DOCUMENT_LENGTH)\n",
        "        # Weight variables and RNN cell\n",
        "        # 1st convolution and pooling layers\n",
        "        self.conv1 = layers.Conv2D(N_FILTERS, FILTER_SHAPE1, padding='VALID', activation='relu', use_bias=True)\n",
        "        self.pool1 = layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
        "        # 2nd convolution and pooling layers\n",
        "        self.conv2 = layers.Conv2D(N_FILTERS, FILTER_SHAPE2, padding='VALID', activation='relu', use_bias=True)\n",
        "        self.pool2 = layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.dense = layers.Dense(MAX_LABEL, activation='softmax')\n",
        "\n",
        "    def call(self, x, drop_rate=0.5):\n",
        "        # forward\n",
        "        # x - [n_samples, features]\n",
        "        x = self.embedding(x)     # x - [n_samples, features, embedding_size(20)]\n",
        "        x = x[..., tf.newaxis]    # x - [n_samples, features, embedding_size(20), channels(1)]\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        if self.use_dropout:\n",
        "          x = tf.nn.dropout(x, drop_rate)\n",
        "        logits = self.dense(x)\n",
        "        return logits\n",
        "\n",
        "word_cnn_model = WordCNN(vocab_size, False)\n",
        "word_cnn_dropout = WordCNN(vocab_size, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXezMvQJ2nmQ"
      },
      "source": [
        "### Optimiser, Loss, Train, Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdnWpaxEoOiJ"
      },
      "source": [
        "# Choose optimizer and loss function for training\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model. \n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzxUyKI7oRZ2"
      },
      "source": [
        "# Training function\n",
        "def train_step(model, x, label, drop_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        out = model(x, drop_rate)\n",
        "        loss = loss_object(label, out)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, out)\n",
        "\n",
        "# Testing function\n",
        "def test_step(model, x, label, drop_rate=0):\n",
        "    out = model(x,drop_rate)\n",
        "    t_loss = loss_object(label, out)\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(label, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KqvGKRP2rNc"
      },
      "source": [
        "### Train without dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB5Sw2SJoXPE"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(word_cnn_model, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(word_cnn_model, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "word_cnn_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/word_cnn/word_cnn_base'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(word_cnn_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2soiyTT3tO9"
      },
      "source": [
        "# Plot test accuracy\n",
        "pylab.figure(figsize=(12,4))\n",
        "pylab.subplot(1,2,1)\n",
        "pylab.plot(np.arange(no_epochs), test_acc, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_acc, label='train')\n",
        "pylab.title('Model Accuracy for WordCNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('accuracy')\n",
        "pylab.legend(loc='lower right')\n",
        "\n",
        "pylab.subplot(1,2,2)\n",
        "pylab.plot(np.arange(no_epochs), test_ec, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_ec, label='train')\n",
        "pylab.title('Model Loss for WordCNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('entropy cost')\n",
        "pylab.legend(loc='upper right')\n",
        "\n",
        "pylab.savefig('./results/partb/word_cnn/word_cnn_base.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehQ3z2GL2v_l"
      },
      "source": [
        "### Train with dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mqveTL724b9"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(word_cnn_dropout, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(word_cnn_dropout, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "word_cnn_dropout_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = '.results/partb/word_cnn/word_cnn_dropout'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(word_cnn_dropout_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahgsk0ejpzFx"
      },
      "source": [
        "# Plot test accuracy\n",
        "pylab.figure(figsize=(12,4))\n",
        "pylab.subplot(1,2,1)\n",
        "pylab.plot(np.arange(no_epochs), test_acc, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_acc, label='train')\n",
        "pylab.title('Model Accuracy for WordCNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('accuracy')\n",
        "pylab.legend(loc='lower right')\n",
        "\n",
        "pylab.subplot(1,2,2)\n",
        "pylab.plot(np.arange(no_epochs), test_ec, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_ec, label='train')\n",
        "pylab.title('Model Loss for WordCNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('entropy cost')\n",
        "pylab.legend(loc='upper right')\n",
        "\n",
        "pylab.savefig('.results/partb/word_cnn/word_cnn_dropout.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UApPtiiErXYV"
      },
      "source": [
        "## WordRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uj4D-N_sHXc"
      },
      "source": [
        "MAX_DOCUMENT_LENGTH = 100\n",
        "HIDDEN_SIZE = 20\n",
        "EMBEDDING_SIZE = 20\n",
        "\n",
        "if not os.path.exists('./results/partb/word_rnn'):\n",
        "    os.mkdir('./results/partb/word_rnn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrNnkQ2arSBU"
      },
      "source": [
        "# Build model\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "class WordRNN(Model):\n",
        "    def __init__(self, vocab_size, hidden_dim=10, use_dropout=False, cell_type='gru', stacked=False):\n",
        "        super(WordRNN, self).__init__()\n",
        "        # Hyperparameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.use_dropout = use_dropout\n",
        "        self.stacked = stacked\n",
        "        self.embedding = layers.Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_DOCUMENT_LENGTH)\n",
        "        # Weight variables and RNN cell\n",
        "        if cell_type == 'vanilla':\n",
        "          self.rnn = layers.RNN(tf.keras.layers.SimpleRNNCell(self.hidden_dim), \n",
        "                                unroll=True)\n",
        "        elif cell_type == 'gru':\n",
        "          if stacked:\n",
        "            self.rnn1 = layers.RNN(tf.keras.layers.GRUCell(self.hidden_dim), \n",
        "                                   unroll=True, return_sequences=True)\n",
        "            self.rnn2 = layers.RNN(tf.keras.layers.GRUCell(self.hidden_dim), \n",
        "                                   unroll=True)\n",
        "          else:\n",
        "            self.rnn = layers.RNN(tf.keras.layers.GRUCell(self.hidden_dim), \n",
        "                                   unroll=True)\n",
        "        elif cell_type == 'lstm':\n",
        "          self.rnn = layers.RNN(tf.keras.layers.LSTMCell(self.hidden_dim), \n",
        "                                   unroll=True)\n",
        "\n",
        "        self.dense = layers.Dense(MAX_LABEL, activation='softmax')\n",
        "\n",
        "    def call(self, x, drop_rate):\n",
        "        # forward logic\n",
        "        embedding = self.embedding(x)   # x - [n_samples, features, embedding_size(20)]\n",
        "        if self.stacked:\n",
        "          encoding = self.rnn1(embedding)\n",
        "          encoding = self.rnn2(encoding)\n",
        "        else:\n",
        "          encoding = self.rnn(embedding)\n",
        "        \n",
        "        if self.use_dropout:\n",
        "          encoding = tf.nn.dropout(encoding, drop_rate)\n",
        "        logits = self.dense(encoding)\n",
        "    \n",
        "        return logits\n",
        "\n",
        "word_rnn_model = WordRNN(vocab_size, HIDDEN_SIZE, False)\n",
        "word_rnn_dropout = WordRNN(vocab_size, HIDDEN_SIZE, True)\n",
        "word_rnn_vanilla = WordRNN(vocab_size, HIDDEN_SIZE, False, cell_type='vanilla')\n",
        "word_rnn_lstm = WordRNN(vocab_size, HIDDEN_SIZE, False, cell_type='lstm')\n",
        "word_rnn_stacked = WordRNN(vocab_size, HIDDEN_SIZE, False, stacked=True)\n",
        "word_rnn_gradclip = WordRNN(vocab_size, HIDDEN_SIZE, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whmdErAQ3TUq"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO7BjHM-rZCn"
      },
      "source": [
        "# Choose optimizer and loss function for training\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model. \n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q2d36b_s6B6"
      },
      "source": [
        "# Training function\n",
        "def train_step(model, x, label, drop_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        out = model(x, drop_rate)\n",
        "        loss = loss_object(label, out)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, out)\n",
        "\n",
        "# Testing function\n",
        "def test_step(model, x, label, drop_rate=0):\n",
        "    out = model(x,drop_rate)\n",
        "    t_loss = loss_object(label, out)\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(label, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYM5A1lt3WsH"
      },
      "source": [
        "### Train without dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Io8WBjs_zu"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(word_rnn_model, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(word_rnn_model, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "word_rnn_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/word_rnnword_rnn_base'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(word_rnn_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gydPTWAv4UQ1"
      },
      "source": [
        "# Plot test accuracy\n",
        "pylab.figure(figsize=(12,4))\n",
        "pylab.subplot(1,2,1)\n",
        "pylab.plot(np.arange(no_epochs), test_acc, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_acc, label='train')\n",
        "pylab.title('Model Accuracy for WordRNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('accuracy')\n",
        "pylab.legend(loc='lower right')\n",
        "\n",
        "pylab.subplot(1,2,2)\n",
        "pylab.plot(np.arange(no_epochs), test_ec, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_ec, label='train')\n",
        "pylab.title('Model Loss for WordRNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('entropy cost')\n",
        "pylab.legend(loc='upper right')\n",
        "\n",
        "pylab.savefig('./results/partb/word_rnn/word_rnn_base.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez5rcimc3Y3n"
      },
      "source": [
        "### Train with dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBbBWB2O3cyO"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(word_rnn_dropout, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(word_rnn_dropout, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "word_rnn_dropout_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/word_rnn/word_rnn_dropout'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(word_rnn_dropout_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRqVD7B32f9n"
      },
      "source": [
        "# Plot test accuracy\n",
        "pylab.figure(figsize=(12,4))\n",
        "pylab.subplot(1,2,1)\n",
        "pylab.plot(np.arange(no_epochs), test_acc, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_acc, label='train')\n",
        "pylab.title('Model Accuracy for WordRNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('accuracy')\n",
        "pylab.legend(loc='lower right')\n",
        "\n",
        "pylab.subplot(1,2,2)\n",
        "pylab.plot(np.arange(no_epochs), test_ec, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_ec, label='train')\n",
        "pylab.title('Model Loss for WordRNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('entropy cost')\n",
        "pylab.legend(loc='upper right')\n",
        "\n",
        "pylab.savefig('./results/partb/word_rnn/word_rnn_dropout.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ByIqMiMuQCx"
      },
      "source": [
        "## CharRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C468jw41uTEX"
      },
      "source": [
        "MAX_DOCUMENT_LENGTH = 100\n",
        "HIDDEN_SIZE = 20\n",
        "\n",
        "if not os.path.exists('./results/partb/char_rnn'):\n",
        "    os.mkdir('./results/partb/char_rnn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOv5CtT3uYln"
      },
      "source": [
        "# Build model\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "class CharRNN(Model):\n",
        "    def __init__(self, vocab_size=256, hidden_dim=10, use_dropout=False, cell_type='gru', stacked=False):\n",
        "        super(CharRNN, self).__init__()\n",
        "        # Hyperparameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.use_dropout = use_dropout\n",
        "        self.cell_type = cell_type\n",
        "        self.stacked = stacked\n",
        "        \n",
        "        # Weight variables and RNN cell\n",
        "        if cell_type == 'vanilla':\n",
        "          self.rnn = layers.RNN(tf.keras.layers.SimpleRNNCell(self.hidden_dim), \n",
        "                                unroll=True)\n",
        "        elif cell_type == 'gru':\n",
        "          if stacked:\n",
        "            self.rnn1 = layers.RNN(tf.keras.layers.GRUCell(self.hidden_dim), \n",
        "                                   unroll=True, return_sequences=True)\n",
        "            self.rnn2 = layers.RNN(tf.keras.layers.GRUCell(self.hidden_dim), \n",
        "                                   unroll=True)\n",
        "          else:\n",
        "            self.rnn = layers.RNN(tf.keras.layers.GRUCell(self.hidden_dim), \n",
        "                                   unroll=True)\n",
        "        elif cell_type == 'lstm':\n",
        "          self.rnn = layers.RNN(tf.keras.layers.LSTMCell(self.hidden_dim), \n",
        "                                   unroll=True)\n",
        "\n",
        "        self.dense = layers.Dense(MAX_LABEL, activation='softmax')\n",
        "\n",
        "    def call(self, x, drop_rate):\n",
        "        # forward logic\n",
        "        x = tf.one_hot(x, one_hot_size)\n",
        "        if self.stacked:\n",
        "          encoding = self.rnn1(x)\n",
        "          encoding = self.rnn2(encoding)\n",
        "        else:\n",
        "          encoding = self.rnn(x)\n",
        "        if self.use_dropout:\n",
        "          encoding = tf.nn.dropout(encoding, drop_rate)\n",
        "        logits = self.dense(encoding)\n",
        "    \n",
        "        return logits\n",
        "\n",
        "char_rnn_model = CharRNN(256, HIDDEN_SIZE, False)\n",
        "char_rnn_dropout = CharRNN(256, HIDDEN_SIZE, True)\n",
        "char_rnn_vanilla = CharRNN(256, HIDDEN_SIZE, False, cell_type='vanilla')\n",
        "char_rnn_lstm = CharRNN(256, HIDDEN_SIZE, False, cell_type='lstm')\n",
        "char_rnn_stacked = CharRNN(256, HIDDEN_SIZE, False, stacked=True)\n",
        "char_rnn_gradclip = CharRNN(256, HIDDEN_SIZE, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SJLk39210UV"
      },
      "source": [
        "### Optimiser, Loss, Train, Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb20VKSyu7LL"
      },
      "source": [
        "# Choose optimizer and loss function for training\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model. \n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8wXYMF0u_pg"
      },
      "source": [
        "# Training function\n",
        "def train_step(model, x, label, drop_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        out = model(x, drop_rate)\n",
        "        loss = loss_object(label, out)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, out)\n",
        "\n",
        "# Testing function\n",
        "def test_step(model, x, label, drop_rate=0):\n",
        "    out = model(x,drop_rate)\n",
        "    t_loss = loss_object(label, out)\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(label, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqKpbFNp13pV"
      },
      "source": [
        "### Train without dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp0lT7HuvDD3"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(char_rnn_model, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(char_rnn_model, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "char_rnn_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/char_rnn/char_rnn_base'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(char_rnn_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug_caUHT4q_W"
      },
      "source": [
        "# Plot test accuracy\n",
        "pylab.figure(figsize=(12,4))\n",
        "pylab.subplot(1,2,1)\n",
        "pylab.plot(np.arange(no_epochs), test_acc, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_acc, label='train')\n",
        "pylab.title('Model Accuracy for CharRNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('accuracy')\n",
        "pylab.legend(loc='lower right')\n",
        "\n",
        "pylab.subplot(1,2,2)\n",
        "pylab.plot(np.arange(no_epochs), test_ec, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_ec, label='train')\n",
        "pylab.title('Model Loss for CharRNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('entropy cost')\n",
        "pylab.legend(loc='upper right')\n",
        "\n",
        "pylab.savefig('./results/partb/char_rnn/char_rnn_base.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5LcJmnn1_tC"
      },
      "source": [
        "### Train with dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjMB7xKb2CqW"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(char_rnn_dropout, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(char_rnn_dropout, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "char_rnn_dropout_res = {'train_ec': train_ec,\n",
        "                        'train_acc': train_acc,\n",
        "                        'test_ec': test_ec,\n",
        "                        'test_acc': test_acc,\n",
        "                        'timings': timings}\n",
        "\n",
        "filename = './results/partb/char_rnn/char_rnn_dropout'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(char_rnn_dropout_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDTbtGvxFXt1"
      },
      "source": [
        "# Plot test accuracy\n",
        "pylab.figure(figsize=(12,4))\n",
        "pylab.subplot(1,2,1)\n",
        "pylab.plot(np.arange(no_epochs), test_acc, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_acc, label='train')\n",
        "pylab.title('Model Accuracy for CharRNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('accuracy')\n",
        "pylab.legend(loc='lower right')\n",
        "\n",
        "pylab.subplot(1,2,2)\n",
        "pylab.plot(np.arange(no_epochs), test_ec, label='test')\n",
        "pylab.plot(np.arange(no_epochs), train_ec, label='train')\n",
        "pylab.title('Model Loss for CharRNN')\n",
        "pylab.xlabel('epochs')\n",
        "pylab.ylabel('entropy cost')\n",
        "pylab.legend(loc='upper right')\n",
        "\n",
        "pylab.savefig('./results/partb/char_rnn/char_rnn_dropout.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJV4KCmE2YhR"
      },
      "source": [
        "# Question 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CejyB-NQwu81"
      },
      "source": [
        "# Load back dictionaries\n",
        "models = ['char_cnn', 'word_cnn', 'char_rnn', 'word_rnn']\n",
        "all_res = []\n",
        "\n",
        "for model in models:\n",
        "  filename = './results/partb/{}/{}_base'.format(model)\n",
        "  all_res.append(pickle.load(open(filename, \"rb\")))\n",
        "\n",
        "print('Average Time Per Epoch')\n",
        "for res, model in zip(all_res, models):\n",
        "  avg = np.mean(res['timings'])\n",
        "  print('{}: {}'.format(model, avg))\n",
        "\n",
        "print('Max Validation Accuracy')\n",
        "for res, model in zip(all_res, models):\n",
        "  max = np.max(res['test_acc'])\n",
        "  print('{}: {}'.format(model, max))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQQz6MV_qUjq"
      },
      "source": [
        "# Load back dictionaries with dropout\n",
        "models = ['char_cnn', 'word_cnn', 'char_rnn', 'word_rnn']\n",
        "all_res_dropouts = []\n",
        "\n",
        "for model in models:\n",
        "  filename = './results/partb/{}/{}_dropout'.format(model)\n",
        "  all_res_dropouts.append(pickle.load(open(filename, \"rb\")))\n",
        "\n",
        "print('Max Validation Accuracy (with dropouts)')\n",
        "for res, model in zip(all_res_dropouts, models):\n",
        "  max = np.max(res['test_acc'])\n",
        "  print('{}: {}'.format(model, max))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coiN8azWrWRb"
      },
      "source": [
        "# Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb51umH8xAv1"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBVOLhX4rX0g"
      },
      "source": [
        "# Choose optimizer and loss function for training\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model. \n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "# Training function\n",
        "def train_step(model, x, label, drop_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        out = model(x, drop_rate)\n",
        "        loss = loss_object(label, out)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, out)\n",
        "\n",
        "# Testing function\n",
        "def test_step(model, x, label, drop_rate=0):\n",
        "    out = model(x,drop_rate)\n",
        "    t_loss = loss_object(label, out)\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(label, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMJkkpVsrjqS"
      },
      "source": [
        "### CharRNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFr78CL8xIPM"
      },
      "source": [
        "#### Train for Vanilla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECgCLuTQxDHk"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds_c:\n",
        "        train_step(char_rnn_vanilla, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds_c:\n",
        "        test_step(char_rnn_vanilla, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "char_rnn_vanilla_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/char_rnn/char_rnn_vanilla'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(char_rnn_vanilla_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J922B5AuxbKZ"
      },
      "source": [
        "#### Train for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N9fAtPQxdyC"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds_c:\n",
        "        train_step(char_rnn_lstm, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds_c:\n",
        "        test_step(char_rnn_lstm, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "char_rnn_lstm_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/char_rnn/char_rnn_lstm'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(char_rnn_lstm_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MHCQzW5xy61"
      },
      "source": [
        "#### Train for 2 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTeJaJWcxx27"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds_c:\n",
        "        train_step(char_rnn_stacked, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds_c:\n",
        "        test_step(char_rnn_stacked, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "char_rnn_stacked_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/char_rnn/char_rnn_stacked'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(char_rnn_stacked_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsJh9Lj_yBlE"
      },
      "source": [
        "#### Train with gradient clipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeXTLkevyOwS"
      },
      "source": [
        "# Choose optimizer and loss function for training\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, clipvalue=2)\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model. \n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "# Training function\n",
        "def train_step(model, x, label, drop_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        out = model(x, drop_rate)\n",
        "        loss = loss_object(label, out)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, out)\n",
        "\n",
        "# Testing function\n",
        "def test_step(model, x, label, drop_rate=0):\n",
        "    out = model(x,drop_rate)\n",
        "    t_loss = loss_object(label, out)\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(label, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiQ5zUyCyDko"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds_c:\n",
        "        train_step(char_rnn_gradclip, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds_c:\n",
        "        test_step(char_rnn_gradclip, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "char_rnn_gradclip_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/char_rnn/char_rnn_gradclip'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(char_rnn_gradclip_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r8COmH06FEE"
      },
      "source": [
        "### Load back results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dELzVOje3B4J"
      },
      "source": [
        "# Load back dictionaries with dropout\n",
        "char_rnn_models = ['vanilla', 'lstm', 'stacked', 'gradclip']\n",
        "all_char_rnn_res = []\n",
        "\n",
        "for model in char_rnn_models:\n",
        "  filename = './results/partb/char_rnn/char_rnn_{}'.format(model)\n",
        "  all_char_rnn_res.append(pickle.load(open(filename, \"rb\")))\n",
        "\n",
        "print('Max Validation Accuracy for CharRNN Models')\n",
        "for res, model in zip(all_char_rnn_res, char_rnn_models):\n",
        "  max = np.max(res['test_acc'])\n",
        "  print('{}: {}'.format(model, max))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ3wwxBmnXTL"
      },
      "source": [
        "## WordRNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlnAxvK2nq3K"
      },
      "source": [
        "### Train for Vanilla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak-yzfFang6_"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(word_rnn_vanilla, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(word_rnn_vanilla, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "word_rnn_vanilla_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/word_rnn/word_rnn_vanilla'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(word_rnn_vanilla_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4DseX1Np6s-"
      },
      "source": [
        "### Train for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0n9gp4Ap8dg"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(word_rnn_lstm, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(word_rnn_lstm, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "word_rnn_lstm_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/word_rnn/word_rnn_lstm'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(word_rnn_lstm_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grL6nytZqFwN"
      },
      "source": [
        "### Train for 2 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3luKiIwqH3c"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(word_rnn_stacked, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(word_rnn_stacked, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "word_rnn_stacked_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/word_rnn/word_rnn_stacked'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(word_rnn_stacked_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRZAClhZqPaE"
      },
      "source": [
        "### Train for gradient clipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIZhQqsCqd4R"
      },
      "source": [
        "# Choose optimizer and loss function for training\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, clipvalue=2)\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model. \n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "# Training function\n",
        "def train_step(model, x, label, drop_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        out = model(x, drop_rate)\n",
        "        loss = loss_object(label, out)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, out)\n",
        "\n",
        "# Testing function\n",
        "def test_step(model, x, label, drop_rate=0):\n",
        "    out = model(x,drop_rate)\n",
        "    t_loss = loss_object(label, out)\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(label, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoLYra9yqRER"
      },
      "source": [
        "train_ec = []\n",
        "train_acc = []\n",
        "test_ec = []\n",
        "test_acc = []\n",
        "timings = []\n",
        "\n",
        "for epoch in range(no_epochs):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    start_time = time()\n",
        "    for images, labels in train_ds:\n",
        "        train_step(word_rnn_gradclip, images, labels, drop_rate=0.5)\n",
        "    \n",
        "    end_time = time()\n",
        "    timings.append(end_time-start_time)\n",
        "\n",
        "    for images, labels in test_ds:\n",
        "        test_step(word_rnn_gradclip, images, labels, drop_rate=0)\n",
        "\n",
        "    train_ec.append(train_loss.result())\n",
        "    train_acc.append(train_accuracy.result())\n",
        "    test_ec.append(test_loss.result())\n",
        "    test_acc.append(test_accuracy.result())\n",
        "    \n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "    print (template.format(epoch+1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result(),\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result()))\n",
        "\n",
        "word_rnn_gradclip_res = {'train_ec': train_ec,\n",
        "                'train_acc': train_acc,\n",
        "                'test_ec': test_ec,\n",
        "                'test_acc': test_acc,\n",
        "                'timings': timings}\n",
        "\n",
        "filename = './results/partb/word_rnn/word_rnn_gradclip'\n",
        "with open(filename, 'wb') as file_pi:\n",
        "      pickle.dump(word_rnn_gradclip_res, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm03OcMNq1MO"
      },
      "source": [
        "### Load Back Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_x4favOq28K"
      },
      "source": [
        "# Load back dictionaries with dropout\n",
        "word_rnn_models = ['vanilla', 'lstm', 'stacked', 'gradclip']\n",
        "all_word_rnn_res = []\n",
        "\n",
        "for model in word_rnn_models:\n",
        "  filename = './results/partb/word_rnn/word_rnn_{}'.format(model)\n",
        "  all_word_rnn_res.append(pickle.load(open(filename, \"rb\")))\n",
        "\n",
        "print('Max Validation Accuracy for WordRNN Models')\n",
        "for res, model in zip(all_word_rnn_res, word_rnn_models):\n",
        "  max = np.max(res['test_acc'])\n",
        "  print('{}: {}'.format(model, max))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}